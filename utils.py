from __future__ import print_function
import numpy as np
import os
import networkx as nx

def potential_paths_from_source_to_target(G, source, target):
    """
    return the subgraph that only contains the useful edges to connect source to target in a shortest path problem
    """
    G2 = G.copy()
    for (u,v) in G2.edges:
        G2[u][v]['weight']=0
    new_edge = True
    while new_edge:
        new_edge = False
        path = nx.dijkstra_path(G2, source, target)
        for i in range(len(path)-1):
            new_edge = (new_edge or G2[path[i]][path[i+1]]['weight']==0) # we visited a new edge
            G2[path[i]][path[i+1]]['weight'] +=1
    for (u,v) in G.edges:
        if G2[u][v]['weight']==0:
            G2.remove_edge(u,v)
    for n in G.nodes:
        if not(nx.has_path(G2, source, n)):
            G2.remove_node(n)
    return G2

def simu(mab, rew, oracle, algo, horizon):
    """
    Simulate an instance for t = 1 , ... , horizon with the setting given by mab (MAB class), rew (Reward class), oracle (Oracle class) and algo (BanditAlgo class).
    Return the evolution of the regret generated by each timestep, as well as the history of pulls of the algo
    """
    pulls = []
    reward = np.zeros(horizon)
    X = mab.simu(steps=horizon) # generated statistics

    for t in range(horizon):
        plays = algo.action() # arms pulled
        feedback = rew.feedback(X[t], plays) # semi bandit feedback
        reward[t] = rew.reward(mab.means, plays) # pseudo reward
        algo.update(plays, feedback)  # update algo
        pulls.append(plays)

    baseline = rew.reward(mab.means, oracle.action(mab.means)) # the best achievable reward per timestep
    return (np.cumsum(baseline-reward), pulls)

if __name__ == '__main__':
    pass