from __future__ import print_function
import numpy as np
import os
import networkx as nx
import time

def potential_paths_from_source_to_target(G, source, target):
    """
    return the subgraph that only contains the useful edges to connect source to target in a shortest path problem
    """
    G2 = G.copy()
    for (u,v) in G2.edges:
        G2[u][v]['weight']=0
    new_edge = True
    while new_edge:
        new_edge = False
        path = nx.dijkstra_path(G2, source, target)
        for i in range(len(path)-1):
            new_edge = (new_edge or G2[path[i]][path[i+1]]['weight']==0) # we visited a new edge
            G2[path[i]][path[i+1]]['weight'] +=1
    for (u,v) in G.edges:
        if G2[u][v]['weight']==0:
            G2.remove_edge(u,v)
    for n in G.nodes:
        if not(nx.has_path(G2, source, n)):
            G2.remove_node(n)
    return G2

def simu(mab, rew, oracle, algo, horizon):
    """
    Simulate an instance for t = 1 , ... , horizon with the setting given by mab (MAB class), rew (Reward class), oracle (Oracle class) and algo (BanditAlgo class).
    Return the evolution of the regret generated by each timestep, as well as the history of pulls of the algo
    """
    pulls = []
    reward = np.zeros(horizon)
    X = mab.simu(steps=horizon) # generated statistics

    for t in range(horizon):
        plays = algo.action() # arms pulled
        feedback = rew.feedback(X[t], plays) # semi bandit feedback
        reward[t] = rew.reward(mab.means, plays) # pseudo reward
        algo.update(plays, feedback)  # update algo
        pulls.append(plays)

    baseline = rew.reward(mab.means, oracle.action(mab.means)) # the best achievable reward per timestep
    return (np.cumsum(baseline-reward), pulls)

def runtime(mab, rew, oracle, algo, horizon):
    """
    Returns the average time for the algo to compute algo.action() and algo.update().
    """
    try:
        if algo.init:
            check_init = True
        else:
            check_init = False
    except AttributeError:
        check_init = False
    if check_init:
        X = mab.simu(steps=2*horizon) # generated statistics
    else:
        X = mab.simu(steps=horizon)
    n=0
    t=0
    actiontime = 0.
    updatetime = 0.
    while n<horizon: # n is the number of timesteps after the initialization (we do not count the initialisation)
        _actiontime = time.time()
        plays = algo.action() # arms pulled
        if not(check_init):
            actiontime += time.time() - _actiontime
        feedback = rew.feedback(X[t], plays) # semi bandit feedback
        _updatetime = time.time()
        algo.update(plays, feedback)  # update algo
        if not(check_init):
            updatetime += time.time() - _updatetime
            n += 1
        if check_init:
            check_init = algo.init
        t += 1

    return actiontime/n, updatetime/n


if __name__ == '__main__':
    pass