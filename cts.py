from __future__ import print_function
import numpy as np
import os

### The code is really factorized so it is easy to implement new types of reward functions/variable distributions, and so on.
    
class MAB():
    """
    Superclass for different types of MAB setting. 
    A MAB must have a self.simu(steps=n) function which returns n iid samples (X_t)_t where X_t is in R^K. 
    """
    def __init__(self, distrib):
        self.distrib = distrib


class GaussianCombMAB(MAB):
    """
    Bandits with multivariate gaussian reward distributions.
    """

    def __init__(self, means, cov):
        """
        Initialize with the mean and the covariance of the multivariate distribution.
        """
        super().__init__("MultivariateGaussian")
        self.means = means
        self.cov = cov

    def simu(self, steps=1):
        """
        Simulate a variable X ~ N(means, cov)
        """
        return np.random.multivariate_normal(self.means, self.cov, size=steps)


def simu(mab, rew, oracle, algo, horizon):
    """
    Simulate an instance for t = 1 , ... , horizon with the setting given by mab (MAB class), rew (Reward class), oracle (Oracle class) and algo (BanditAlgo class).
    Return the evolution of the regret generated by each timestep, as well as the history of pulls of the algo
    """
    pulls = []
    reward = np.zeros(horizon)
    X = mab.simu(steps=horizon) # generated statistics

    for t in range(horizon):
        plays = algo.action() # arms pulled
        algo.update(plays, X[t, plays]) # update the algo

        #reward[t] = rew.reward(X, plays) # actual reward
        reward[t] = rew.reward(mab.means, plays) # pseudo reward
        pulls.append(plays)

    baseline = rew.reward(mab.means, oracle.action(mab.means)) # the best achievable reward per timestep
    return (np.cumsum(baseline-reward), pulls)

if __name__ == '__main__':
    pass